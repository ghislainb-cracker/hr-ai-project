{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3103a-7514-4ed6-9e1f-32a133057de0",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\n================================================================================\nMAYA FALLBACK ENDPOINT - PRODUCTION DEPLOYMENT\n================================================================================\nVersion: 2.0 (Production Grade)\nPurpose: WhatsApp HR AI Bot - Fallback endpoint for max 10 users\nInstance: ml.g5.2xlarge (1x NVIDIA A10G, 24GB VRAM)\nRegion: ap-south-1 (Mumbai)\n\nRun cells in order: 1 -> 2 -> 3 -> 4\n================================================================================\n\"\"\"\n\nimport boto3\nimport json\nimport time\nimport logging\nfrom datetime import datetime, timedelta\nfrom botocore.exceptions import ClientError, BotoCoreError\nfrom botocore.config import Config as BotoConfig\nfrom typing import Optional, Dict, Any, Tuple\nfrom functools import wraps\n\n# =============================================================================\n# LOGGING SETUP (Production-grade)\n# =============================================================================\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='[%(asctime)s] [%(levelname)s] %(message)s',\n    datefmt='%H:%M:%S'\n)\nlogger = logging.getLogger(\"MayaDeployment\")\n\n# =============================================================================\n# CONFIGURATION\n# =============================================================================\n\nclass Config:\n    \"\"\"Centralized configuration for Maya fallback endpoint.\"\"\"\n    \n    # Endpoint Settings\n    ENDPOINT_NAME = \"maya-prod-mumbai-1768132592\"\n    REGION = \"ap-south-1\"\n    VARIANT_NAME = \"AllTraffic\"\n    \n    # Instance Configuration\n    INSTANCE_TYPE = \"ml.g5.2xlarge\"\n    \n    # Scaling Limits\n    MIN_INSTANCES = 1\n    MAX_INSTANCES = 2\n    \n    # Scaling Behavior (Conservative for fallback)\n    INVOCATIONS_TARGET = 150.0\n    SCALE_OUT_COOLDOWN = 300   # 5 min\n    SCALE_IN_COOLDOWN = 600    # 10 min\n    \n    # Model Location\n    MODEL_S3_URI = \"s3://sagemaker-ap-south-1-937127308917/maya-prod-v1-bf16/model.tar.gz\"\n    \n    # Retry Configuration\n    MAX_RETRIES = 3\n    RETRY_DELAY_BASE = 1.0  # seconds\n    RETRY_DELAY_MAX = 30.0  # seconds\n    \n    # Timeouts\n    API_TIMEOUT_CONNECT = 10  # seconds\n    API_TIMEOUT_READ = 30     # seconds\n\n# =============================================================================\n# RETRY DECORATOR (Exponential Backoff)\n# =============================================================================\n\ndef retry_with_backoff(max_retries: int = Config.MAX_RETRIES, \n                       base_delay: float = Config.RETRY_DELAY_BASE,\n                       max_delay: float = Config.RETRY_DELAY_MAX):\n    \"\"\"\n    Decorator that retries a function with exponential backoff.\n    Handles transient AWS failures gracefully.\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            last_exception = None\n            \n            for attempt in range(max_retries + 1):\n                try:\n                    return func(*args, **kwargs)\n                except (ClientError, BotoCoreError) as e:\n                    last_exception = e\n                    \n                    # Don't retry on permission/validation errors\n                    if isinstance(e, ClientError):\n                        error_code = e.response.get('Error', {}).get('Code', '')\n                        non_retryable = ['ValidationException', 'AccessDeniedException', \n                                        'InvalidParameterException', 'ResourceNotFoundException']\n                        if error_code in non_retryable:\n                            raise\n                    \n                    if attempt < max_retries:\n                        delay = min(base_delay * (2 ** attempt), max_delay)\n                        logger.warning(f\"Attempt {attempt + 1} failed: {e}. Retrying in {delay:.1f}s...\")\n                        time.sleep(delay)\n                    else:\n                        logger.error(f\"All {max_retries + 1} attempts failed\")\n                        raise\n                        \n            return None\n        return wrapper\n    return decorator\n\n# =============================================================================\n# AWS CLIENTS (Thread-safe with retry config)\n# =============================================================================\n\nclass AWSClients:\n    \"\"\"Production-grade AWS client manager with connection pooling and retries.\"\"\"\n    \n    _clients: Dict[str, Any] = {}\n    _boto_config = BotoConfig(\n        connect_timeout=Config.API_TIMEOUT_CONNECT,\n        read_timeout=Config.API_TIMEOUT_READ,\n        retries={'max_attempts': 3, 'mode': 'adaptive'}\n    )\n    \n    @classmethod\n    def _get_client(cls, service_name: str):\n        \"\"\"Get or create a boto3 client with proper configuration.\"\"\"\n        if service_name not in cls._clients:\n            cls._clients[service_name] = boto3.client(\n                service_name, \n                region_name=Config.REGION,\n                config=cls._boto_config\n            )\n        return cls._clients[service_name]\n    \n    @classmethod\n    def sagemaker(cls):\n        return cls._get_client('sagemaker')\n    \n    @classmethod\n    def runtime(cls):\n        return cls._get_client('sagemaker-runtime')\n    \n    @classmethod\n    def autoscaling(cls):\n        return cls._get_client('application-autoscaling')\n    \n    @classmethod\n    def cloudwatch(cls):\n        return cls._get_client('cloudwatch')\n    \n    @classmethod\n    def reset(cls):\n        \"\"\"Reset all clients (useful for error recovery).\"\"\"\n        cls._clients.clear()\n\n# =============================================================================\n# HELPER FUNCTIONS\n# =============================================================================\n\ndef get_resource_id() -> str:\n    \"\"\"Returns the auto-scaling resource ID for the endpoint.\"\"\"\n    return f\"endpoint/{Config.ENDPOINT_NAME}/variant/{Config.VARIANT_NAME}\"\n\ndef log_success(msg: str):\n    logger.info(f\"[OK] {msg}\")\n\ndef log_error(msg: str):\n    logger.error(f\"[ERR] {msg}\")\n\ndef log_warn(msg: str):\n    logger.warning(f\"[!] {msg}\")\n\ndef log_info(msg: str):\n    logger.info(f\"[>] {msg}\")\n\n# =============================================================================\n# ENDPOINT STATUS CHECKER\n# =============================================================================\n\n@retry_with_backoff()\ndef get_endpoint_status() -> Tuple[str, int]:\n    \"\"\"\n    Get endpoint status and instance count.\n    Returns: (status, instance_count)\n    Raises: Exception if endpoint not found\n    \"\"\"\n    resp = AWSClients.sagemaker().describe_endpoint(EndpointName=Config.ENDPOINT_NAME)\n    status = resp[\"EndpointStatus\"]\n    \n    instance_count = 0\n    for variant in resp.get(\"ProductionVariants\", []):\n        if variant[\"VariantName\"] == Config.VARIANT_NAME:\n            instance_count = variant.get(\"CurrentInstanceCount\", 0)\n            break\n    \n    return status, instance_count\n\n@retry_with_backoff()\ndef get_scaling_config() -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Get current scaling configuration.\n    Returns: Dict with min/max capacity or None if not configured\n    \"\"\"\n    resp = AWSClients.autoscaling().describe_scalable_targets(\n        ServiceNamespace='sagemaker',\n        ResourceIds=[get_resource_id()],\n        ScalableDimension='sagemaker:variant:DesiredInstanceCount'\n    )\n    \n    if resp[\"ScalableTargets\"]:\n        target = resp[\"ScalableTargets\"][0]\n        return {\n            \"min_capacity\": target[\"MinCapacity\"],\n            \"max_capacity\": target[\"MaxCapacity\"]\n        }\n    return None\n\n@retry_with_backoff()\ndef get_scaling_policies() -> list:\n    \"\"\"Get all scaling policies for the endpoint.\"\"\"\n    resp = AWSClients.autoscaling().describe_scaling_policies(\n        ServiceNamespace='sagemaker',\n        ResourceId=get_resource_id(),\n        ScalableDimension='sagemaker:variant:DesiredInstanceCount'\n    )\n    return resp.get(\"ScalingPolicies\", [])\n\n# =============================================================================\n# INITIALIZATION\n# =============================================================================\n\nprint(\"=\" * 70)\nprint(\"MAYA FALLBACK ENDPOINT - PRODUCTION DEPLOYMENT\")\nprint(\"=\" * 70)\nlog_success(\"Configuration loaded\")\nlog_info(f\"Endpoint: {Config.ENDPOINT_NAME}\")\nlog_info(f\"Region: {Config.REGION}\")\nlog_info(f\"Scaling: Min={Config.MIN_INSTANCES}, Max={Config.MAX_INSTANCES}\")\nlog_info(f\"Scale trigger: >{Config.INVOCATIONS_TARGET} invocations/min per instance\")\nlog_info(f\"Retry config: {Config.MAX_RETRIES} retries with exponential backoff\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thune87brc",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\n================================================================================\nCELL 2: PRE-FLIGHT CHECKS\n================================================================================\nComprehensive validation before making any changes.\nSafe to run multiple times.\n\"\"\"\n\ndef preflight_check() -> Dict[str, Any]:\n    \"\"\"\n    Production-grade pre-flight validation.\n    \n    Checks:\n    1. Endpoint exists and is InService\n    2. Current scaling configuration\n    3. Existing policies (flags dangerous ones)\n    4. Instance count sanity\n    \n    Returns: Dict with all findings and issues\n    \"\"\"\n    \n    results = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"endpoint_exists\": False,\n        \"endpoint_status\": None,\n        \"current_instances\": 0,\n        \"scaling_config\": None,\n        \"scaling_policies\": [],\n        \"issues\": [],\n        \"warnings\": [],\n        \"ready_for_fix\": False\n    }\n    \n    print(\"=\" * 70)\n    print(\"PRE-FLIGHT CHECK\")\n    print(\"=\" * 70)\n    \n    # -------------------------------------------------------------------------\n    # CHECK 1: Endpoint Status\n    # -------------------------------------------------------------------------\n    log_info(\"Checking endpoint status...\")\n    try:\n        status, instances = get_endpoint_status()\n        results[\"endpoint_exists\"] = True\n        results[\"endpoint_status\"] = status\n        results[\"current_instances\"] = instances\n        \n        if status == \"InService\":\n            log_success(f\"Endpoint InService with {instances} instance(s)\")\n        elif status == \"Updating\":\n            log_warn(f\"Endpoint is Updating - wait for it to complete\")\n            results[\"warnings\"].append(\"Endpoint is currently updating\")\n        else:\n            log_error(f\"Endpoint status: {status}\")\n            results[\"issues\"].append(f\"Endpoint not InService: {status}\")\n            \n    except ClientError as e:\n        if \"Could not find endpoint\" in str(e):\n            log_error(\"Endpoint does not exist!\")\n            results[\"issues\"].append(\"Endpoint not found - needs to be created first\")\n        else:\n            log_error(f\"AWS Error: {e}\")\n            results[\"issues\"].append(f\"AWS Error: {str(e)}\")\n        print(\"=\" * 70)\n        return results\n    except Exception as e:\n        log_error(f\"Unexpected error: {e}\")\n        results[\"issues\"].append(f\"Unexpected error: {str(e)}\")\n        print(\"=\" * 70)\n        return results\n    \n    # -------------------------------------------------------------------------\n    # CHECK 2: Scaling Configuration\n    # -------------------------------------------------------------------------\n    log_info(\"Checking auto-scaling configuration...\")\n    try:\n        scaling = get_scaling_config()\n        results[\"scaling_config\"] = scaling\n        \n        if scaling:\n            min_cap = scaling[\"min_capacity\"]\n            max_cap = scaling[\"max_capacity\"]\n            log_info(f\"Current scaling: Min={min_cap}, Max={max_cap}\")\n            \n            if min_cap > Config.MIN_INSTANCES:\n                results[\"issues\"].append(\n                    f\"MinCapacity={min_cap} is higher than target {Config.MIN_INSTANCES}\"\n                )\n            if max_cap > Config.MAX_INSTANCES:\n                results[\"issues\"].append(\n                    f\"MaxCapacity={max_cap} is higher than target {Config.MAX_INSTANCES}\"\n                )\n            if min_cap == Config.MIN_INSTANCES and max_cap == Config.MAX_INSTANCES:\n                log_success(\"Scaling limits are correctly configured\")\n        else:\n            log_warn(\"No auto-scaling configured\")\n            results[\"warnings\"].append(\"Auto-scaling not configured\")\n            \n    except Exception as e:\n        log_warn(f\"Could not check scaling config: {e}\")\n        results[\"warnings\"].append(f\"Scaling check failed: {str(e)}\")\n    \n    # -------------------------------------------------------------------------\n    # CHECK 3: Scaling Policies\n    # -------------------------------------------------------------------------\n    log_info(\"Checking scaling policies...\")\n    try:\n        policies = get_scaling_policies()\n        results[\"scaling_policies\"] = [p[\"PolicyName\"] for p in policies]\n        \n        for policy in policies:\n            policy_name = policy[\"PolicyName\"]\n            config = policy.get(\"TargetTrackingScalingPolicyConfiguration\", {})\n            \n            # Check for dangerous latency-based policy\n            custom_metric = config.get(\"CustomizedMetricSpecification\", {})\n            if custom_metric.get(\"MetricName\") == \"ModelLatency\" or \"Latency\" in policy_name:\n                log_error(f\"DANGEROUS: Latency-based policy '{policy_name}'\")\n                results[\"issues\"].append(\n                    f\"Latency policy '{policy_name}' causes phantom scaling - MUST REMOVE\"\n                )\n            else:\n                log_info(f\"Policy: {policy_name}\")\n            \n            # Check cooldowns\n            scale_out = config.get(\"ScaleOutCooldown\", 0)\n            if scale_out < 60:\n                results[\"issues\"].append(\n                    f\"Policy '{policy_name}' has aggressive ScaleOutCooldown={scale_out}s\"\n                )\n                \n        if not policies:\n            log_info(\"No scaling policies found\")\n            \n    except Exception as e:\n        log_warn(f\"Could not check policies: {e}\")\n        results[\"warnings\"].append(f\"Policy check failed: {str(e)}\")\n    \n    # -------------------------------------------------------------------------\n    # CHECK 4: Instance Count Sanity\n    # -------------------------------------------------------------------------\n    if results[\"current_instances\"] > Config.MAX_INSTANCES:\n        results[\"issues\"].append(\n            f\"OVER-PROVISIONED: {results['current_instances']} instances running \"\n            f\"(max should be {Config.MAX_INSTANCES})\"\n        )\n        log_error(f\"Too many instances: {results['current_instances']}\")\n    \n    # -------------------------------------------------------------------------\n    # SUMMARY\n    # -------------------------------------------------------------------------\n    print(\"\\n\" + \"=\" * 70)\n    print(\"SUMMARY\")\n    print(\"=\" * 70)\n    \n    if results[\"issues\"]:\n        log_error(f\"Found {len(results['issues'])} issue(s):\")\n        for i, issue in enumerate(results[\"issues\"], 1):\n            print(f\"   {i}. {issue}\")\n    \n    if results[\"warnings\"]:\n        log_warn(f\"Found {len(results['warnings'])} warning(s):\")\n        for i, warn in enumerate(results[\"warnings\"], 1):\n            print(f\"   {i}. {warn}\")\n    \n    if not results[\"issues\"] and not results[\"warnings\"]:\n        log_success(\"All checks passed - endpoint is healthy!\")\n    \n    # Determine if we can proceed with fix\n    results[\"ready_for_fix\"] = (\n        results[\"endpoint_status\"] == \"InService\" and \n        len([i for i in results[\"issues\"] if \"not found\" in i.lower()]) == 0\n    )\n    \n    if results[\"issues\"] and results[\"ready_for_fix\"]:\n        print(\"\\n\" + \"-\" * 70)\n        log_info(\"Run Cell 3 to fix the issues above\")\n    \n    print(\"=\" * 70)\n    return results\n\n# Execute pre-flight check\npreflight_results = preflight_check()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zppmnv1gjkl",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\n================================================================================\nCELL 3: APPLY BULLETPROOF AUTO-SCALING\n================================================================================\nProduction-grade scaling configuration that WILL NOT crash.\n\nFeatures:\n- Removes ALL dangerous policies (latency-based)\n- Sets hard limits (Min=1, Max=4)\n- Single conservative policy\n- Exponential backoff retries\n- Idempotent (safe to run multiple times)\n- Automatic scale-down if over-provisioned\n================================================================================\n\"\"\"\n\n@retry_with_backoff()\ndef _delete_scaling_policy(policy_name: str, resource_id: str) -> bool:\n    \"\"\"Delete a single scaling policy with retry.\"\"\"\n    AWSClients.autoscaling().delete_scaling_policy(\n        PolicyName=policy_name,\n        ServiceNamespace='sagemaker',\n        ResourceId=resource_id,\n        ScalableDimension='sagemaker:variant:DesiredInstanceCount'\n    )\n    return True\n\n@retry_with_backoff()\ndef _register_scaling_target(resource_id: str, min_cap: int, max_cap: int) -> bool:\n    \"\"\"Register scalable target with retry.\"\"\"\n    AWSClients.autoscaling().register_scalable_target(\n        ServiceNamespace='sagemaker',\n        ResourceId=resource_id,\n        ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n        MinCapacity=min_cap,\n        MaxCapacity=max_cap\n    )\n    return True\n\n@retry_with_backoff()\ndef _put_scaling_policy(resource_id: str) -> bool:\n    \"\"\"Apply the conservative scaling policy with retry.\"\"\"\n    AWSClients.autoscaling().put_scaling_policy(\n        PolicyName='Maya-Fallback-Conservative',\n        ServiceNamespace='sagemaker',\n        ResourceId=resource_id,\n        ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n        PolicyType='TargetTrackingScaling',\n        TargetTrackingScalingPolicyConfiguration={\n            'TargetValue': Config.INVOCATIONS_TARGET,\n            'PredefinedMetricSpecification': {\n                'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance'\n            },\n            'ScaleInCooldown': Config.SCALE_IN_COOLDOWN,\n            'ScaleOutCooldown': Config.SCALE_OUT_COOLDOWN,\n            'DisableScaleIn': False\n        }\n    )\n    return True\n\n@retry_with_backoff()\ndef _force_scale_down(target_instances: int) -> bool:\n    \"\"\"Force endpoint to scale down with retry.\"\"\"\n    AWSClients.sagemaker().update_endpoint_weights_and_capacities(\n        EndpointName=Config.ENDPOINT_NAME,\n        DesiredWeightsAndCapacities=[{\n            'VariantName': Config.VARIANT_NAME,\n            'DesiredInstanceCount': target_instances\n        }]\n    )\n    return True\n\n\ndef fix_autoscaling() -> bool:\n    \"\"\"\n    Apply bulletproof auto-scaling configuration.\n    \n    This function is:\n    - Idempotent: Safe to run multiple times\n    - Resilient: Retries transient failures\n    - Safe: Won't break a working endpoint\n    \n    Returns: True if successful, False otherwise\n    \"\"\"\n    \n    print(\"=\" * 70)\n    print(\"APPLYING BULLETPROOF AUTO-SCALING CONFIGURATION\")\n    print(\"=\" * 70)\n    \n    resource_id = get_resource_id()\n    success = True\n    \n    # =========================================================================\n    # STEP 1: Verify endpoint is ready\n    # =========================================================================\n    log_info(\"Step 1/5: Verifying endpoint status...\")\n    try:\n        status, current_instances = get_endpoint_status()\n        \n        if status != \"InService\":\n            log_error(f\"Endpoint is '{status}' - cannot modify. Wait for InService.\")\n            return False\n            \n        log_success(f\"Endpoint InService with {current_instances} instance(s)\")\n        \n    except Exception as e:\n        log_error(f\"Failed to verify endpoint: {e}\")\n        return False\n    \n    # =========================================================================\n    # STEP 2: Remove ALL existing scaling policies\n    # =========================================================================\n    log_info(\"Step 2/5: Removing existing scaling policies...\")\n    try:\n        policies = get_scaling_policies()\n        removed = 0\n        \n        for policy in policies:\n            policy_name = policy[\"PolicyName\"]\n            try:\n                _delete_scaling_policy(policy_name, resource_id)\n                log_info(f\"   Removed: {policy_name}\")\n                removed += 1\n            except Exception as e:\n                log_warn(f\"   Could not remove {policy_name}: {e}\")\n        \n        if removed > 0:\n            log_success(f\"Removed {removed} policy(ies)\")\n        else:\n            log_info(\"No existing policies to remove\")\n            \n    except Exception as e:\n        log_warn(f\"Error checking policies: {e}\")\n    \n    # =========================================================================\n    # STEP 3: Set scaling limits\n    # =========================================================================\n    log_info(f\"Step 3/5: Setting limits (Min={Config.MIN_INSTANCES}, Max={Config.MAX_INSTANCES})...\")\n    try:\n        _register_scaling_target(resource_id, Config.MIN_INSTANCES, Config.MAX_INSTANCES)\n        log_success(f\"Scaling limits configured\")\n    except Exception as e:\n        log_error(f\"Failed to set scaling limits: {e}\")\n        return False\n    \n    # =========================================================================\n    # STEP 4: Apply conservative scaling policy\n    # =========================================================================\n    log_info(\"Step 4/5: Applying conservative scaling policy...\")\n    try:\n        _put_scaling_policy(resource_id)\n        log_success(f\"Policy applied: scale at >{Config.INVOCATIONS_TARGET} invocations/min\")\n        log_info(f\"   Scale-out cooldown: {Config.SCALE_OUT_COOLDOWN}s\")\n        log_info(f\"   Scale-in cooldown: {Config.SCALE_IN_COOLDOWN}s\")\n    except Exception as e:\n        log_error(f\"Failed to apply policy: {e}\")\n        return False\n    \n    # =========================================================================\n    # STEP 5: Force scale-down if needed\n    # =========================================================================\n    if current_instances > Config.MAX_INSTANCES:\n        log_info(f\"Step 5/5: Scaling down from {current_instances} to {Config.MIN_INSTANCES}...\")\n        try:\n            _force_scale_down(Config.MIN_INSTANCES)\n            log_success(f\"Scale-down initiated (takes 5-10 min to complete)\")\n        except Exception as e:\n            log_warn(f\"Could not force scale-down: {e}\")\n            log_info(\"The new policy will handle this automatically\")\n    else:\n        log_success(f\"Step 5/5: Instance count OK ({current_instances} <= {Config.MAX_INSTANCES})\")\n    \n    # =========================================================================\n    # COMPLETE\n    # =========================================================================\n    print(\"\\n\" + \"=\" * 70)\n    print(\"CONFIGURATION COMPLETE\")\n    print(\"=\" * 70)\n    log_success(\"Auto-scaling is now bulletproof:\")\n    print(f\"\"\"\n   Configuration Applied:\n   ----------------------\n   Min instances:        {Config.MIN_INSTANCES}\n   Max instances:        {Config.MAX_INSTANCES} (HARD CAP)\n   Scale-out trigger:    >{Config.INVOCATIONS_TARGET} invocations/min per instance\n   Scale-out cooldown:   {Config.SCALE_OUT_COOLDOWN}s (5 min)\n   Scale-in cooldown:    {Config.SCALE_IN_COOLDOWN}s (10 min)\n   Latency scaling:      DISABLED (was causing issues)\n   \n   Safety Features:\n   ----------------\n   - Exponential backoff retries on all AWS calls\n   - Single policy (no conflicts)\n   - High threshold (won't trigger for 10 users)\n   - Long cooldowns (prevents rapid scaling)\n    \"\"\")\n    print(\"-\" * 70)\n    log_info(\"Run Cell 4 to verify endpoint health\")\n    print(\"=\" * 70)\n    \n    return True\n\n\n# Execute the fix\nfix_success = fix_autoscaling()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cik2b6c74o",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\n================================================================================\nCELL 4: HEALTH CHECK & INFERENCE TEST\n================================================================================\nComprehensive verification that everything is working.\n\"\"\"\n\n@retry_with_backoff()\ndef _test_inference() -> Tuple[bool, float, str]:\n    \"\"\"\n    Run a test inference with retry.\n    Returns: (success, latency_ms, response_text)\n    \"\"\"\n    test_prompt = (\n        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n        \"You are Maya. Reply with exactly one word.<|eot_id|>\"\n        \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n        \"Hi<|eot_id|>\"\n        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n    )\n    \n    payload = {\n        \"inputs\": test_prompt,\n        \"parameters\": {\n            \"max_new_tokens\": 10,\n            \"temperature\": 0.1,\n            \"do_sample\": False,\n            \"return_full_text\": False\n        }\n    }\n    \n    start = time.time()\n    response = AWSClients.runtime().invoke_endpoint(\n        EndpointName=Config.ENDPOINT_NAME,\n        ContentType=\"application/json\",\n        Body=json.dumps(payload)\n    )\n    latency_ms = (time.time() - start) * 1000\n    \n    result = json.loads(response[\"Body\"].read().decode())\n    generated = result[0][\"generated_text\"] if isinstance(result, list) else result[\"generated_text\"]\n    \n    return True, latency_ms, generated.strip()\n\n\ndef health_check() -> Dict[str, Any]:\n    \"\"\"\n    Comprehensive health check with test inference.\n    \n    Checks:\n    1. Endpoint status\n    2. Scaling configuration matches target\n    3. No dangerous policies\n    4. Inference works\n    \n    Returns: Dict with all results\n    \"\"\"\n    \n    results = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"checks_passed\": 0,\n        \"checks_failed\": 0,\n        \"endpoint_healthy\": False,\n        \"scaling_correct\": False,\n        \"inference_working\": False,\n        \"inference_latency_ms\": None,\n        \"details\": {}\n    }\n    \n    print(\"=\" * 70)\n    print(\"HEALTH CHECK\")\n    print(\"=\" * 70)\n    \n    # -------------------------------------------------------------------------\n    # CHECK 1: Endpoint Status\n    # -------------------------------------------------------------------------\n    log_info(\"Check 1/4: Endpoint status...\")\n    try:\n        status, instances = get_endpoint_status()\n        results[\"details\"][\"endpoint_status\"] = status\n        results[\"details\"][\"instance_count\"] = instances\n        \n        if status == \"InService\":\n            log_success(f\"InService with {instances} instance(s)\")\n            results[\"checks_passed\"] += 1\n            results[\"endpoint_healthy\"] = True\n        else:\n            log_error(f\"Status: {status}\")\n            results[\"checks_failed\"] += 1\n    except Exception as e:\n        log_error(f\"Failed: {e}\")\n        results[\"checks_failed\"] += 1\n    \n    # -------------------------------------------------------------------------\n    # CHECK 2: Scaling Configuration\n    # -------------------------------------------------------------------------\n    log_info(\"Check 2/4: Scaling configuration...\")\n    try:\n        scaling = get_scaling_config()\n        \n        if scaling:\n            min_ok = scaling[\"min_capacity\"] == Config.MIN_INSTANCES\n            max_ok = scaling[\"max_capacity\"] == Config.MAX_INSTANCES\n            results[\"details\"][\"scaling\"] = scaling\n            \n            if min_ok and max_ok:\n                log_success(f\"Correct: Min={scaling['min_capacity']}, Max={scaling['max_capacity']}\")\n                results[\"checks_passed\"] += 1\n                results[\"scaling_correct\"] = True\n            else:\n                log_warn(f\"Mismatch: Min={scaling['min_capacity']}, Max={scaling['max_capacity']}\")\n                log_info(f\"Expected: Min={Config.MIN_INSTANCES}, Max={Config.MAX_INSTANCES}\")\n                results[\"checks_failed\"] += 1\n        else:\n            log_warn(\"No scaling configured\")\n            results[\"checks_failed\"] += 1\n    except Exception as e:\n        log_error(f\"Failed: {e}\")\n        results[\"checks_failed\"] += 1\n    \n    # -------------------------------------------------------------------------\n    # CHECK 3: No Dangerous Policies\n    # -------------------------------------------------------------------------\n    log_info(\"Check 3/4: Scaling policies...\")\n    try:\n        policies = get_scaling_policies()\n        policy_names = [p[\"PolicyName\"] for p in policies]\n        results[\"details\"][\"policies\"] = policy_names\n        \n        # Check for dangerous latency policies\n        dangerous = []\n        for policy in policies:\n            name = policy[\"PolicyName\"]\n            config = policy.get(\"TargetTrackingScalingPolicyConfiguration\", {})\n            custom = config.get(\"CustomizedMetricSpecification\", {})\n            \n            if custom.get(\"MetricName\") == \"ModelLatency\" or \"Latency\" in name:\n                dangerous.append(name)\n        \n        if dangerous:\n            log_error(f\"DANGEROUS policies found: {dangerous}\")\n            results[\"checks_failed\"] += 1\n        elif len(policies) == 1 and policies[0][\"PolicyName\"] == \"Maya-Fallback-Conservative\":\n            log_success(f\"Correct policy active: {policies[0]['PolicyName']}\")\n            results[\"checks_passed\"] += 1\n        elif len(policies) == 0:\n            log_warn(\"No scaling policies\")\n            results[\"checks_passed\"] += 1  # Not necessarily bad\n        else:\n            log_info(f\"Policies: {policy_names}\")\n            results[\"checks_passed\"] += 1\n    except Exception as e:\n        log_error(f\"Failed: {e}\")\n        results[\"checks_failed\"] += 1\n    \n    # -------------------------------------------------------------------------\n    # CHECK 4: Test Inference\n    # -------------------------------------------------------------------------\n    log_info(\"Check 4/4: Test inference...\")\n    try:\n        success, latency_ms, response = _test_inference()\n        results[\"inference_latency_ms\"] = latency_ms\n        results[\"details\"][\"inference_response\"] = response[:100]\n        \n        log_success(f\"Inference OK: {latency_ms:.0f}ms\")\n        log_info(f\"   Response: \\\"{response[:50]}...\\\"\")\n        results[\"checks_passed\"] += 1\n        results[\"inference_working\"] = True\n        \n    except Exception as e:\n        log_error(f\"Inference failed: {e}\")\n        results[\"checks_failed\"] += 1\n    \n    # -------------------------------------------------------------------------\n    # SUMMARY\n    # -------------------------------------------------------------------------\n    print(\"\\n\" + \"=\" * 70)\n    print(\"RESULTS\")\n    print(\"=\" * 70)\n    \n    total = results[\"checks_passed\"] + results[\"checks_failed\"]\n    \n    if results[\"checks_failed\"] == 0:\n        log_success(f\"ALL {total} CHECKS PASSED\")\n        print(\"\"\"\n   Endpoint Status:  HEALTHY\n   Scaling Config:   CORRECT\n   Policies:         SAFE\n   Inference:        WORKING\n   \n   Your endpoint is production-ready!\n        \"\"\")\n    else:\n        log_error(f\"{results['checks_failed']}/{total} checks failed\")\n        if results[\"checks_passed\"] > 0:\n            log_info(f\"{results['checks_passed']}/{total} checks passed\")\n    \n    print(\"=\" * 70)\n    return results\n\n\n# Run health check\nhealth_results = health_check()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knrzyqvmaes",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\n================================================================================\nCELL 5: EMERGENCY CONTROLS\n================================================================================\nQuick commands for emergency situations.\nAll functions have retry logic and are safe to run.\n\"\"\"\n\ndef quick_status():\n    \"\"\"Quick status check - safe to run anytime.\"\"\"\n    print(\"-\" * 50)\n    print(\"QUICK STATUS\")\n    print(\"-\" * 50)\n    try:\n        status, instances = get_endpoint_status()\n        scaling = get_scaling_config()\n        \n        print(f\"Endpoint:  {Config.ENDPOINT_NAME}\")\n        print(f\"Status:    {status}\")\n        print(f\"Instances: {instances}\")\n        \n        if scaling:\n            print(f\"Scaling:   Min={scaling['min_capacity']}, Max={scaling['max_capacity']}\")\n        else:\n            print(\"Scaling:   Not configured\")\n            \n    except Exception as e:\n        print(f\"Error: {e}\")\n    print(\"-\" * 50)\n\n# Run status check\nquick_status()\n\n\n# =============================================================================\n# EMERGENCY FUNCTIONS (uncomment to use)\n# =============================================================================\n\n@retry_with_backoff()\ndef force_scale_to_one():\n    \"\"\"\n    EMERGENCY: Force endpoint to exactly 1 instance.\n    Use when costs are spiking unexpectedly.\n    \"\"\"\n    print(\"=\" * 50)\n    print(\"EMERGENCY: FORCING SCALE TO 1 INSTANCE\")\n    print(\"=\" * 50)\n    \n    # Lock scaling to 1\n    AWSClients.autoscaling().register_scalable_target(\n        ServiceNamespace='sagemaker',\n        ResourceId=get_resource_id(),\n        ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n        MinCapacity=1,\n        MaxCapacity=1\n    )\n    log_success(\"Scaling locked to Min=1, Max=1\")\n    \n    # Force the scale\n    AWSClients.sagemaker().update_endpoint_weights_and_capacities(\n        EndpointName=Config.ENDPOINT_NAME,\n        DesiredWeightsAndCapacities=[{\n            'VariantName': Config.VARIANT_NAME,\n            'DesiredInstanceCount': 1\n        }]\n    )\n    log_success(\"Scale-down initiated (takes 5-10 min)\")\n    print(\"=\" * 50)\n\n# Uncomment to force scale to 1:\n# force_scale_to_one()\n\n\n@retry_with_backoff()\ndef disable_autoscaling():\n    \"\"\"\n    Completely disable auto-scaling.\n    Endpoint stays at current instance count.\n    \"\"\"\n    print(\"=\" * 50)\n    print(\"DISABLING AUTO-SCALING\")\n    print(\"=\" * 50)\n    \n    # Remove all policies\n    policies = get_scaling_policies()\n    for p in policies:\n        try:\n            _delete_scaling_policy(p[\"PolicyName\"], get_resource_id())\n            log_info(f\"Removed policy: {p['PolicyName']}\")\n        except:\n            pass\n    \n    # Deregister target\n    try:\n        AWSClients.autoscaling().deregister_scalable_target(\n            ServiceNamespace='sagemaker',\n            ResourceId=get_resource_id(),\n            ScalableDimension='sagemaker:variant:DesiredInstanceCount'\n        )\n        log_success(\"Auto-scaling disabled\")\n    except Exception as e:\n        log_warn(f\"Could not deregister: {e}\")\n    \n    print(\"Instance count is now FIXED at current value\")\n    print(\"=\" * 50)\n\n# Uncomment to disable auto-scaling:\n# disable_autoscaling()\n\n\ndef delete_endpoint():\n    \"\"\"\n    NUCLEAR OPTION: Delete the endpoint completely.\n    Stops all billing immediately.\n    Requires explicit confirmation.\n    \"\"\"\n    print(\"=\" * 50)\n    print(\"WARNING: ENDPOINT DELETION\")\n    print(\"=\" * 50)\n    print(f\"Endpoint: {Config.ENDPOINT_NAME}\")\n    print(\"This will PERMANENTLY delete the endpoint.\")\n    print(\"All instances will be terminated.\")\n    print(\"-\" * 50)\n    \n    confirm = input(f\"Type 'DELETE' to confirm: \")\n    \n    if confirm == \"DELETE\":\n        try:\n            AWSClients.sagemaker().delete_endpoint(EndpointName=Config.ENDPOINT_NAME)\n            log_success(\"Endpoint deletion initiated\")\n            print(\"Billing will stop once deletion completes (~5 min)\")\n        except Exception as e:\n            log_error(f\"Deletion failed: {e}\")\n    else:\n        print(\"Cancelled.\")\n    \n    print(\"=\" * 50)\n\n# Uncomment to delete endpoint (requires confirmation):\n# delete_endpoint()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ev8a6pe5",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\n================================================================================\nCELL 6: MONITORING DASHBOARD\n================================================================================\nView endpoint metrics to verify scaling behavior.\n\"\"\"\n\n@retry_with_backoff()\ndef _get_metric(metric_name: str, stat: str, hours: int) -> Optional[list]:\n    \"\"\"Fetch a single metric from CloudWatch.\"\"\"\n    end_time = datetime.utcnow()\n    start_time = end_time - timedelta(hours=hours)\n    \n    resp = AWSClients.cloudwatch().get_metric_statistics(\n        Namespace='AWS/SageMaker',\n        MetricName=metric_name,\n        Dimensions=[\n            {'Name': 'EndpointName', 'Value': Config.ENDPOINT_NAME},\n            {'Name': 'VariantName', 'Value': Config.VARIANT_NAME}\n        ],\n        StartTime=start_time,\n        EndTime=end_time,\n        Period=3600,  # 1 hour\n        Statistics=[stat]\n    )\n    return resp.get('Datapoints', [])\n\n\ndef show_metrics(hours: int = 6):\n    \"\"\"\n    Display endpoint metrics for the last N hours.\n    \n    Shows:\n    - Total invocations\n    - Average latency\n    - Error counts\n    \"\"\"\n    \n    print(\"=\" * 70)\n    print(f\"METRICS: Last {hours} hours\")\n    print(\"=\" * 70)\n    print(f\"Endpoint: {Config.ENDPOINT_NAME}\")\n    print(f\"Region:   {Config.REGION}\")\n    print(\"-\" * 70)\n    \n    metrics = [\n        (\"Invocations\", \"Sum\", \"requests\"),\n        (\"ModelLatency\", \"Average\", \"ms\"),\n        (\"Invocation4XXErrors\", \"Sum\", \"errors\"),\n        (\"Invocation5XXErrors\", \"Sum\", \"errors\")\n    ]\n    \n    for metric_name, stat, unit in metrics:\n        try:\n            datapoints = _get_metric(metric_name, stat, hours)\n            \n            if datapoints:\n                values = [d[stat] for d in datapoints]\n                total = sum(values)\n                avg = total / len(values) if values else 0\n                \n                if metric_name == \"ModelLatency\":\n                    # Convert microseconds to ms\n                    print(f\"{metric_name:25} avg {avg/1000:,.0f} {unit}\")\n                elif \"Error\" in metric_name:\n                    if total > 0:\n                        print(f\"{metric_name:25} {total:,.0f} {unit} (!)\")\n                    else:\n                        print(f\"{metric_name:25} 0 {unit}\")\n                else:\n                    print(f\"{metric_name:25} {total:,.0f} {unit} ({avg:,.1f}/hr)\")\n            else:\n                print(f\"{metric_name:25} No data\")\n                \n        except Exception as e:\n            print(f\"{metric_name:25} Error: {e}\")\n    \n    print(\"-\" * 70)\n    \n    # Show current scaling status\n    try:\n        status, instances = get_endpoint_status()\n        scaling = get_scaling_config()\n        \n        print(f\"Current Status:           {status}\")\n        print(f\"Current Instances:        {instances}\")\n        if scaling:\n            print(f\"Scaling Config:           Min={scaling['min_capacity']}, Max={scaling['max_capacity']}\")\n    except Exception as e:\n        print(f\"Status Error: {e}\")\n    \n    print(\"=\" * 70)\n\n\ndef show_scaling_history(hours: int = 24):\n    \"\"\"\n    Show scaling activity over the last N hours.\n    Helps identify if scaling is behaving correctly.\n    \"\"\"\n    \n    print(\"=\" * 70)\n    print(f\"SCALING ACTIVITY: Last {hours} hours\")\n    print(\"=\" * 70)\n    \n    try:\n        end_time = datetime.utcnow()\n        start_time = end_time - timedelta(hours=hours)\n        \n        # Get invocations per instance (the scaling metric)\n        resp = AWSClients.cloudwatch().get_metric_statistics(\n            Namespace='AWS/SageMaker',\n            MetricName='InvocationsPerInstance',\n            Dimensions=[\n                {'Name': 'EndpointName', 'Value': Config.ENDPOINT_NAME},\n                {'Name': 'VariantName', 'Value': Config.VARIANT_NAME}\n            ],\n            StartTime=start_time,\n            EndTime=end_time,\n            Period=3600,\n            Statistics=['Average', 'Maximum']\n        )\n        \n        datapoints = sorted(resp.get('Datapoints', []), key=lambda x: x['Timestamp'])\n        \n        if datapoints:\n            print(f\"{'Time':20} {'Avg/Instance':15} {'Max/Instance':15}\")\n            print(\"-\" * 50)\n            for dp in datapoints[-12:]:  # Last 12 hours\n                time_str = dp['Timestamp'].strftime('%Y-%m-%d %H:%M')\n                avg = dp.get('Average', 0)\n                max_val = dp.get('Maximum', 0)\n                \n                # Flag if close to scaling threshold\n                flag = \" <-- near threshold\" if avg > Config.INVOCATIONS_TARGET * 0.7 else \"\"\n                print(f\"{time_str:20} {avg:>12.1f}   {max_val:>12.1f}{flag}\")\n        else:\n            print(\"No scaling data available\")\n            \n    except Exception as e:\n        print(f\"Error: {e}\")\n    \n    print(\"=\" * 70)\n\n\n# Show metrics\nshow_metrics(6)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}